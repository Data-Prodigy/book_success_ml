version: '3.8'

services:
  feature_pipeline:
    build:
      context: .
      dockerfile: Dockerfile.hops
    container_name: feature_pipeline
    working_dir: /app
    command: ["python3", "src/features/feature_pipeline.py"]

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: mlflow_server
    command: mlflow server --backend-store-uri file:///mlruns --default-artifact-root /mlruns --host 0.0.0.0 --port 5000
    volumes:
      - ./mlruns:/mlruns
    ports:
      - "5000:5000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 10s
      retries: 12

  training_pipeline:
    build:
      context: .
      dockerfile: Dockerfile.hops
    container_name: training_pipeline
    working_dir: /app
    command: ["python3", "src/features/train_pipeline.py"]
    depends_on:
      - feature_pipeline
      - mlflow
    volumes:
      - ./mlruns:/app/mlruns

  inference:
    build:
      context: .
      dockerfile: Dockerfile.hops
    container_name: inference
    working_dir: /app
    volumes:
      - ./mlruns:/app/mlruns
    command: ["python3", "src/features/inference_pipeline.py"]
    depends_on:
      - training_pipeline

  fastapi:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    container_name: fastapi_app
    working_dir: /app
    volumes:
      - ./mlruns:/app/mlruns
    ports:
      - "8085:8000"
    command: ["python3", "-m", "uvicorn", "api.fast_api:app", "--host", "0.0.0.0", "--port", "8000"]
    depends_on:
      - inference
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8085/docs"]
      interval: 5s
      retries: 12
      timeout: 5s

volumes:
  postgres_data:
